import logging
import time

from openai import APITimeoutError, OpenAI

from app.config import ModelConfig, get_config

config = get_config()

logger = logging.getLogger(__name__)


class TranslationClient:
    """Base class for translation clients."""

    SYSTEM_PROMPT = config.SYSTEM_PROMPT

    def __init__(self, model_config: ModelConfig):
        """Initialize the translation client."""
        self.model_name = model_config["name"]
        self.input_cost_per_mtok = model_config["input_cost_per_mtok"]
        self.output_cost_per_mtok = model_config["output_cost_per_mtok"]
        self.rate_limit = model_config.get("rate_limit")
        self.last_request_time = 0.0
        self.request_count = 0

    def translate(self, text: str) -> tuple[str, float]:
        """
        Translate the given text using the specified model.

        Args:
            text: Text to translate.

        Returns:
            Translated text and the cost of the API call.
        """
        msg = "Subclasses must implement translate()"
        raise NotImplementedError(msg)

    def _calculate_cost(self, input_tokens: float, output_tokens: float) -> float:
        """Calculate the cost of the API call."""
        cost = (
            (input_tokens * self.input_cost_per_mtok)
            + (output_tokens * self.output_cost_per_mtok)
        ) / 1_000_000
        logger.debug(
            f"Cost calculation for {self.model_name}: "
            f"input_tokens={input_tokens}, output_tokens={output_tokens}, cost={cost}"
        )
        return cost

    def _check_rate_limit(self) -> bool:
        """Check if the rate limit has been exceeded."""
        if not self.rate_limit:
            return True

        current_time = time.time()
        # Default rate limit window is 60 seconds
        rate_limit_window = 60
        if current_time - self.last_request_time < rate_limit_window:
            if self.request_count >= self.rate_limit:
                return False
            self.request_count += 1
        else:
            self.last_request_time = current_time
            self.request_count = 1
        return True


class OpenRouterClient(TranslationClient):
    """Client for OpenRouter models."""

    def __init__(self, model_config: ModelConfig):
        """Initialize the OpenRouter client."""
        super().__init__(model_config)
        self.reasoning = model_config.get("reasoning")
        self.custom_temperature = model_config.get("temperature")
        self.timeout = model_config.get("timeout", 90.0)  # Thinking models use 180s

    def translate(self, text: str) -> tuple[str, float]:
        """Translate text using the OpenRouter API."""
        if not config.OPENROUTER_API_KEY:
            return "Error: API key not configured for OpenRouter", 0.0

        try:
            client = OpenAI(
                base_url=config.OPENROUTER_BASE_URL,
                api_key=config.OPENROUTER_API_KEY,
            )

            # Extra body parameters for reasoning models
            extra_body = {}
            if self.reasoning:
                extra_body["reasoning"] = self.reasoning

            completion = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": text},
                ],
                temperature=self.custom_temperature
                if self.custom_temperature is not None
                else config.DEFAULT_TEMPERATURE,
                extra_body=extra_body if extra_body else None,
                timeout=self.timeout,
            )

            logger.info(
                f"Full OpenRouter Response for {self.model_name}: {completion!r}"
            )

            if not completion.choices:
                logger.warning(
                    f"OpenRouter response for {self.model_name} had no choices."
                )
                return "Error: No response generated by model.", 0.0

            choice = completion.choices[0]

            # Check for finish reason
            if choice.finish_reason == "length":
                error_msg = "Error: The response was cut off because it reached the maximum token limit."
                logger.warning(f"{error_msg} for model {self.model_name}")
                return error_msg, 0.0

            translation = choice.message.content or ""

            usage = completion.usage
            input_tokens = (
                usage.prompt_tokens
                if usage and usage.prompt_tokens is not None
                else len(text) / 4
            )
            output_tokens = (
                usage.completion_tokens
                if usage and usage.completion_tokens is not None
                else len(translation) / 4
            )

            cost = self._calculate_cost(input_tokens, output_tokens)

        except APITimeoutError:
            error_msg = f"Error: Request timed out for model {self.model_name}."
            logger.error(error_msg)
            return error_msg, 0.0

        except Exception as e:
            logger.exception(
                f"OpenRouter translation failed for {self.model_name}: {e!s}"
            )
            return f"Error: {e!s}", 0.0
        else:
            logger.info(f"OpenRouter translation successful for {self.model_name}.")
            return translation, cost


def get_translation_client(model_key: str) -> TranslationClient:
    """
    Create a translation client for the given model key.

    Args:
        model_key: The key of the model in the configuration.

    Returns:
        A translation client instance.

    Raises:
        ValueError: If the model key or type is unknown.
    """
    model_config = config.MODELS.get(model_key)
    if not model_config:
        msg = f"Unknown model: {model_key}"
        raise ValueError(msg)

    # Simplified logic: All active models essentially use OpenRouter now as per requirement.
    # But check type just in case.
    model_type = model_config.get("type", "openrouter")

    if model_type == "openrouter":
        return OpenRouterClient(model_config)

    # Fallback or error for now? User said "Remove Gemini Client... use OpenRouter for all models".
    # So we force OpenRouterClient even if type says gemini (if we updated config correctly, type is openrouter).
    # But if config still has "gemini" type for old models and we didn't update them all?
    # I updated config to set type="openrouter" for all Gemini models.
    # So this should be safe.

    if model_type == "gemini":
        # Fallback to OpenRouterClient as GeminiClient is removed
        logger.warning(
            f"Model {model_key} has type 'gemini' but GeminiClient is removed. Using OpenRouterClient."
        )
        return OpenRouterClient(model_config)

    msg = f"Unsupported model type: {model_type}"
    raise ValueError(msg)


def get_available_models() -> dict[str, str]:
    """Get a dictionary of available, active models."""
    return {
        key: model["display_name"]
        for key, model in config.MODELS.items()
        if model.get("is_active", True)
    }
